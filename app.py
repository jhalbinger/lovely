from flask import Flask, request, jsonify
import openai
import os
from dotenv import load_dotenv
import json
from collections import defaultdict, deque
import requests

# === CONFIGURACIONES ===
load_dotenv()

api_key = os.getenv("OPENAI_API_KEY")
project_id = os.getenv("OPENAI_PROJECT_ID")
organization_id = os.getenv("OPENAI_ORG_ID")

client = openai.OpenAI(
    api_key=api_key,
    project=project_id,
    organization=organization_id
)

app = Flask(__name__)

# === CARGAR TODO EL CONTEXTO UNA SOLA VEZ ===
txt_path = "lovely_taller.txt"
if os.path.exists(txt_path):
    with open(txt_path, "r", encoding="utf-8") as f:
        CONTEXTO_COMPLETO = f.read()
else:
    CONTEXTO_COMPLETO = ""

# Memoria por usuario: √∫ltimas 4 interacciones para GPT
historial_conversacion = defaultdict(lambda: deque(maxlen=4))

# Estado de cada usuario: puede ser "esperando_confirmacion" o "derivado"
estado_usuario = {}
# √öltimo producto consultado por usuario
producto_usuario = {}

@app.route("/webhook", methods=["POST"])
def responder():
    try:
        datos = request.get_json()
        print("üîé JSON recibido desde WhatsApp/Twilio:")
        print(json.dumps(datos, indent=2))

        mensaje_usuario = datos.get("consulta", "")
        user_id = datos.get("user_id", "anon").strip()

        if not mensaje_usuario:
            return jsonify({"error": "No se recibi√≥ ninguna consulta"}), 400

        # === Si ya fue derivado en esta sesi√≥n, no derivar de nuevo ===
        if estado_usuario.get(user_id) == "derivado":
            return responder_normal(mensaje_usuario, user_id)

        # === Si est√° esperando confirmaci√≥n de derivaci√≥n ===
        if estado_usuario.get(user_id) == "esperando_confirmacion":
            if mensaje_usuario.lower() in ["s√≠", "si", "dale", "ok", "quiero", "confirmo"]:
                # ‚úÖ Derivar solo una vez por sesi√≥n
                estado_usuario[user_id] = "derivado"

                # Producto consultado
                producto = producto_usuario.get(user_id, "No especificado")

                mensaje_para_due√±o = (
                    f"üì© Usuario {user_id} pidi√≥ hablar con un asesor.\n"
                    f"üõãÔ∏è Producto consultado: {producto}"
                )

                try:
                    resp = requests.post(
                        "https://derivacion-humano.onrender.com/derivar-humano",
                        json={"numero": user_id, "consulta": mensaje_para_due√±o}
                    )
                    if resp.status_code == 200:
                        respuesta_llm = "‚úÖ Listo, ya avis√© a un asesor para que te contacte. Mientras tanto, cualquier consulta segu√≠ escribi√©ndome por ac√° que sigo a disposici√≥n üòâ"
                    else:
                        print("‚ùå Error Twilio:", resp.text)
                        respuesta_llm = "‚ùå Intent√© derivarte, pero hubo un problema. Pod√©s llamar al 011 6028‚Äë1211 para coordinar directo."
                except Exception as e:
                    print("‚ùå Error al derivar:", e)
                    respuesta_llm = "‚ùå No pude avisar al asesor en este momento. Pod√©s llamar al 011 6028‚Äë1211 para coordinar directo."

                return jsonify({"respuesta": respuesta_llm})

            else:
                # Si dice "no" o algo distinto, cancela derivaci√≥n
                estado_usuario.pop(user_id, None)
                return jsonify({"respuesta": "üëå Sin problema, cualquier cosa pod√©s consultarme por ac√° cuando quieras."})

        # === Intentamos detectar si menciona un producto en esta consulta ===
        prod_detectado = detectar_producto_mencionado(mensaje_usuario)
        if prod_detectado:
            producto_usuario[user_id] = prod_detectado

        # === Flujo normal con GPT ===
        respuesta_normal = responder_normal(mensaje_usuario, user_id)

        # Contamos cu√°ntas consultas ha hecho este usuario
        consultas_usuario = [msg for rol, msg in historial_conversacion[user_id] if rol == "user"]
        cantidad_consultas = len(consultas_usuario)

        # Si ya hizo al menos 3 consultas, despu√©s de responder le ofrecemos derivaci√≥n
        if cantidad_consultas >= 3 and estado_usuario.get(user_id) != "derivado":
            estado_usuario[user_id] = "esperando_confirmacion"
            extra = "\n\n‚úÖ *Si quer√©s, puedo pedir que un asesor te contacte para coordinar la compra. ¬øQuer√©s que te llame?*"
            respuesta_data = json.loads(respuesta_normal.get_data())
            respuesta_data["respuesta"] += extra
            return jsonify(respuesta_data)

        return respuesta_normal

    except Exception as e:
        print("üí• Error detectado:", e)
        return jsonify({"respuesta": "Estoy tardando en procesar tu consulta, intent√° de nuevo en unos segundos üôè"}), 200


def responder_normal(mensaje_usuario, user_id):
    """Flujo original de GPT para respuestas normales"""
    # PROMPT ESPECIAL PARA WHATSAPP
    system_prompt = (
        "Sos un asistente virtual de *Lovely Taller Deco* üõãÔ∏è. "
        "Respond√© solo con la informaci√≥n del CONTEXTO, no inventes nada. "
        "\n\n"
        "‚û°Ô∏è **Formato WhatsApp:**\n"
        "- Us√° *un solo asterisco* para resaltar palabras clave (productos, precios, direcciones).\n"
        "- Us√° ‚úÖ para listas y agreg√° SALTOS DE L√çNEA entre frases para que el mensaje no quede en un solo bloque.\n"
        "- Cada 1 o 2 frases, cort√° y pon√© un salto de l√≠nea.\n"
        "- Si hay un link, ponelo solo en una l√≠nea para que WhatsApp muestre la vista previa.\n"
        "- M√°ximo 2 emojis por respuesta.\n"
        "\n"
        "‚û°Ô∏è **Extensi√≥n del mensaje:**\n"
        "- Respuesta breve pero completa, ideal para leer en celular (m√°ximo 4-5 l√≠neas de texto).\n"
        "- Si es una lista, m√°ximo 4-5 √≠tems por respuesta.\n"
        "- Despu√©s de responder, suger√≠ UN solo tema l√≥gico para seguir.\n"
        "\n"
        "‚û°Ô∏è **Comportamiento:**\n"
        "- En la PRIMERA respuesta salud√°: '¬°Hola! üëã *Bienvenido a Lovely Taller Deco* üõãÔ∏è‚ú®' y explic√° brevemente qu√© puede consultar.\n"
        "- En mensajes posteriores NO vuelvas a saludar, respond√© directo.\n"
        "- Si ya diste showroom o ubicaci√≥n en la misma conversaci√≥n, no los repitas salvo que lo pidan.\n"
        "- Si la consulta no est√° en el CONTEXTO, no inventes; invit√° a visitar el showroom üè† o llamar al 011 6028‚Äë1211.\n"
    )

    # Historial para GPT
    historial = list(historial_conversacion[user_id])
    mensajes_historial = [{"role": rol, "content": msg} for rol, msg in historial]
    mensajes_historial.append({"role": "user", "content": mensaje_usuario})

    # CONTEXTO + HISTORIAL
    user_prompt = f"CONTEXTO:\n{CONTEXTO_COMPLETO}\n\nConversaci√≥n previa:\n\n"
    for rol, msg in historial:
        user_prompt += f"{rol.upper()}: {msg}\n"
    user_prompt += f"\nUSUARIO (nuevo): {mensaje_usuario}"

    respuesta = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
    )

    respuesta_llm = respuesta.choices[0].message.content.strip()

    # Guardamos historial
    historial_conversacion[user_id].append(("user", mensaje_usuario))
    historial_conversacion[user_id].append(("bot", respuesta_llm))

    return jsonify({"respuesta": respuesta_llm})


def detectar_producto_mencionado(texto):
    """Busca si el mensaje menciona un producto espec√≠fico del contexto"""
    productos = [
        "sill√≥n nube", "sill√≥n roma", "sill√≥n bella", "sill√≥n lady",
        "puff", "esquinero", "mecedora", "respaldo", "silla p√©talo",
        "queen", "estrella", "victoria", "brooklyn", "astor", "diva"
    ]
    texto_lower = texto.lower()
    for p in productos:
        if p in texto_lower:
            return p.title()
    return None


if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000)
